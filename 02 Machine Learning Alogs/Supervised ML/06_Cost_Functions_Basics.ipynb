{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“˜ Cost Function in Machine Learning\n",
        "\n",
        "### What is a Cost Function?\n",
        "A **cost function** is a way to measure how well a machine learning model is doing. It tells us how far off our model's predictions are from the actual values.\n",
        "\n",
        "- If the cost is **low**, the model is doing a good job.\n",
        "- If the cost is **high**, the model needs improvement.\n",
        "\n",
        "We try to **minimize** the cost function so our model performs better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‚ Types of Cost Functions\n",
        "\n",
        "### 1. **Mean Squared Error (MSE)** â€“ for Regression\n",
        "- Measures the average of the squares of errors.\n",
        "- Formula: $( RMSE = \\frac{1}{n} \\sum (y_{true} - y_{pred})^2 $)\n",
        "- **Use case:** When predicting numbers (e.g., house prices).\n",
        "\n",
        "### 2. **Mean Absolute Error (MAE)** â€“ for Regression\n",
        "- Measures the average of absolute differences between actual and predicted values.\n",
        "- Formula: $( MAE = \\frac{1}{n} \\sum |y_{true} - y_{pred}| $)\n",
        "- **Use case:** Similar to MSE, but more robust to outliers.\n",
        "\n",
        "### 1. **Root Mean Squared Error (RMSE)** â€“ for Regression\n",
        "- Measures the average of the squares of errors.\n",
        "- Formula: $( MSE = \\sqrt{\\frac{1}{n} \\sum (y_{true} - y_{pred})^2 } $)\n",
        "- **Use case:** When predicting numbers (e.g., house prices).\n",
        "\n",
        "### 3. **Binary Cross Entropy** â€“ for Binary Classification\n",
        "- Measures how well the predicted probabilities match the true labels (0 or 1).\n",
        "- Formula: $( -[y \\log(p) + (1 - y) \\log(1 - p)] $)\n",
        "- **Use case:** When classifying between two categories (e.g., spam or not).\n",
        "\n",
        "### 4. **Categorical Cross Entropy** â€“ for Multi-class Classification\n",
        "- Used when there are more than two classes.\n",
        "- Compares the predicted probability distribution with the actual class.\n",
        "- **Use case:** Classifying digits (0â€“9), types of animals, etc.\n",
        "\n",
        "### 5. **Hinge Loss** â€“ for SVM Models\n",
        "- Used to train classifiers like Support Vector Machines.\n",
        "- Focuses on maximizing the margin between classes.\n",
        "- **Use case:** Binary classification using SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### âœ… Summary Table\n",
        "\n",
        "| Type                  | Problem Type         | Notes                              |\n",
        "|-----------------------|----------------------|-------------------------------------|\n",
        "| Mean Squared Error    | Regression           | Sensitive to outliers               |\n",
        "| Mean Absolute Error   | Regression           | More robust than MSE                |\n",
        "| Binary Cross Entropy  | Binary Classification| Works with probabilities            |\n",
        "| Categorical Cross Entropy | Multi-class Classification | Softmax output required        |\n",
        "| Hinge Loss            | Classification (SVM) | Focuses on margin                   |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
