{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c648369",
   "metadata": {},
   "source": [
    "# üîÅ Cross-Validation in Machine Learning\n",
    "\n",
    "## üìò Definition\n",
    "\n",
    "**Cross-validation** is a resampling technique used to assess how well a machine learning model generalizes to an independent dataset. It helps to avoid overfitting and gives a better estimate of model performance compared to a single train-test split.\n",
    "\n",
    "In cross-validation, the data is divided into multiple parts (called \"folds\"), and the model is trained and validated multiple times using different subsets of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Purpose of Cross-Validation\n",
    "\n",
    "- Evaluate model performance more reliably\n",
    "- Detect overfitting or underfitting\n",
    "- Optimize hyperparameters with confidence\n",
    "- Ensure the model works well on unseen data\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Types of Cross-Validation\n",
    "\n",
    "### 1. **K-Fold Cross-Validation**\n",
    "\n",
    "**How it works:**\n",
    "- Split the dataset into `k` equal-sized folds.\n",
    "- Train the model on `k-1` folds and test on the remaining fold.\n",
    "- Repeat the process `k` times, each time with a different test fold.\n",
    "- Average the performance over all `k` trials.\n",
    "\n",
    "**Pros:** \n",
    "- More accurate performance estimate than train-test split  \n",
    "**Cons:** \n",
    "- Training is repeated `k` times ‚Üí computational cost\n",
    "\n",
    "**Example:**  \n",
    "If `k = 5`, the dataset is split into 5 parts. The model is trained 5 times, each time leaving one part out for testing.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Stratified K-Fold Cross-Validation** (Classification Data Only)\n",
    "\n",
    "**How it works:**\n",
    "- Like K-Fold, but preserves the percentage of samples for each class.\n",
    "- Especially useful for **imbalanced datasets**.\n",
    "\n",
    "**Pros:** \n",
    "- Balanced class distribution across folds  \n",
    "**Cons:** \n",
    "- Slightly more complex than regular K-Fold\n",
    "\n",
    "**Example:**  \n",
    "If 80% of your data belongs to class A and 20% to class B, each fold will maintain this ratio.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Leave-One-Out Cross-Validation (LOOCV)**\n",
    "\n",
    "**How it works:**\n",
    "- Each sample is used once as the test set, and the remaining `n-1` samples form the training set.\n",
    "- Repeats the process `n` times (where `n` is the number of data points).\n",
    "\n",
    "**Pros:**  \n",
    "- Maximum usage of data for training  \n",
    "**Cons:**  \n",
    "- Very slow for large datasets (training happens `n` times)\n",
    "\n",
    "**Example:**  \n",
    "For a dataset of 100 samples, the model is trained 100 times, each time leaving one sample out for testing.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Leave-P-Out Cross-Validation**\n",
    "\n",
    "**How it works:**\n",
    "- Similar to LOOCV, but leaves out `p` samples for testing instead of 1.\n",
    "- Try all possible combinations of leaving `p` samples out.\n",
    "\n",
    "**Pros:**  \n",
    "- Very thorough  \n",
    "**Cons:**  \n",
    "- Extremely computationally expensive\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Hold-Out Validation**\n",
    "\n",
    "**How it works:**\n",
    "- Simple train/test split (e.g., 80% training, 20% testing).\n",
    "- Not a true cross-validation, but often used as a baseline.\n",
    "\n",
    "**Pros:**  \n",
    "- Fast and simple  \n",
    "**Cons:**  \n",
    "- Performance estimate depends on single split\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Time Series Cross-Validation (Rolling/Sliding Window)**\n",
    "\n",
    "**How it works:**\n",
    "- Used for time-dependent data.\n",
    "- Maintains order of data (no shuffling).\n",
    "- Expands or slides the training window forward in time.\n",
    "\n",
    "**Pros:**  \n",
    "- Respects temporal order  \n",
    "**Cons:**  \n",
    "- Only for time series data\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Summary Table\n",
    "\n",
    "| Type                     | Best For                  | Pros                              | Cons                          |\n",
    "|--------------------------|---------------------------|-----------------------------------|-------------------------------|\n",
    "| K-Fold                   | General use               | Reliable estimate, balanced       | Slower than hold-out          |\n",
    "| Stratified K-Fold        | Imbalanced classification | Preserves class ratio             | Slightly more complex         |\n",
    "| Leave-One-Out (LOOCV)    | Small datasets            | Uses almost all data              | Very slow on large datasets   |\n",
    "| Leave-P-Out              | Research settings          | Very detailed                     | Extremely slow                |\n",
    "| Hold-Out                 | Quick tests               | Fast                              | High variance                 |\n",
    "| Time Series CV           | Time-dependent data       | Keeps time order                  | Not for general datasets      |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Best Practices\n",
    "\n",
    "- Use **Stratified K-Fold** for classification tasks, especially with class imbalance.\n",
    "- Use **K-Fold** (e.g., k=5 or 10) for general-purpose model validation.\n",
    "- Use **Time Series CV** when data has a temporal component.\n",
    "- Avoid LOOCV and Leave-P-Out for large datasets due to cost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d3ea0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
