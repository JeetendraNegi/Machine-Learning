{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9978f8c6",
   "metadata": {},
   "source": [
    "# 🌳 Decision Tree with Entropy and Gini Index 🌳\n",
    "\n",
    "A **decision tree** 🌲 is a supervised machine learning algorithm used for classification and regression tasks. It splits data into subsets based on feature values to create a model that predicts the target variable. The structure of the decision tree consists of nodes representing decisions or tests, and branches that represent outcomes of those decisions.\n",
    "\n",
    "Two commonly used criteria for making splits in decision trees are **entropy** and the **Gini index**. Both are measures of impurity or disorder in a dataset, and they help in determining the best split at each node. Here's a more detailed explanation:\n",
    "\n",
    "## 1. 🔥 Entropy (Information Gain) 🔥\n",
    "\n",
    "**Entropy** 🌀 measures the level of uncertainty or disorder in a dataset.\n",
    "\n",
    "The formula for entropy is:\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(S) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $S$ is the dataset,\n",
    "- $p_i$ is the probability of class $i$ in the dataset.\n",
    "\n",
    "The entropy value ranges from 0 to 1:\n",
    "- An entropy of **0** means the data is pure (all data points belong to a single class).\n",
    "- An entropy of **1** indicates maximum uncertainty (classes are evenly distributed).\n",
    "\n",
    "**Information Gain** 🎯 is the reduction in entropy from a feature split. It is calculated by:\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = \\text{Entropy}(S) - \\sum_{i=1}^{k} \\frac{|S_i|}{|S|} \\text{Entropy}(S_i)\n",
    "$$\n",
    "\n",
    "Where $S_i$ is the subset resulting from the split based on a particular feature.\n",
    "\n",
    "The feature that provides the highest information gain is chosen for splitting the node.\n",
    "\n",
    "## 2. 🌟 Gini Index 🌟\n",
    "\n",
    "**Gini index** 📊 measures the degree of impurity in a dataset. It is a simpler and faster alternative to entropy.\n",
    "\n",
    "The formula for Gini index is:\n",
    "\n",
    "$$\n",
    "\\text{Gini}(S) = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_i$ is the probability of class $i$ in the dataset.\n",
    "\n",
    "A Gini index of **0** means the dataset is pure (all data points belong to a single class).\n",
    "\n",
    "A Gini index of **0.5** indicates maximum disorder (classes are evenly distributed).\n",
    "\n",
    "The **Gini Impurity** 🧑‍🔬 is used to measure the quality of a split. The split with the lowest Gini impurity is chosen.\n",
    "\n",
    "## Differences between Entropy and Gini Index 🤔\n",
    "\n",
    "- **Entropy** 🧠 tends to be more computationally expensive as it involves the logarithm function, while **Gini index** 💨 is simpler and computationally faster.\n",
    "- **Entropy** 🎯 prefers to create balanced splits between classes, while the **Gini index** 🏅 tends to be more biased toward the majority class, though the difference is usually small.\n",
    "\n",
    "## In Summary:\n",
    "\n",
    "- **Entropy** 🌀 measures the amount of disorder or uncertainty in a dataset.\n",
    "- **Gini index** 📊 is another way to measure the impurity of a dataset, often preferred for being computationally simpler.\n",
    "\n",
    "In decision trees 🌳, these criteria help decide how to split the dataset at each node, leading to a model that can make predictions based on the features of the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18df54",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
